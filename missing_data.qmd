---
title: "4. Data in range"
---

Sensible data ranges are often easy to define based on biological principles (elephants are heavy!; mosquitoes are light!) or mathematical principles (percents must be bounded between 0 and 100)

```{r install packages, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
#install packages
packReq <- c("tidyverse",
             "summarytools") 

# Install and load all required packages
lapply(packReq, function(x) {
  print(x)
  if (require(x, character.only = TRUE) == FALSE) {
    install.packages(x)
    library(x, character.only = TRUE)
  }})

```

Load packages

```{r load packages, message=FALSE, warning=FALSE}
library (tidyverse) #general data wrangling
library (summarytools)
```

If you have already completed [module 1](data_types.qmd) you will have created this file. For those who prefer to jump in at the middle, the original dataset with mis-entered dates and missing value codes FIXED is provided here.

```{r read data}
df <-read.csv ('example_data/sunflower_data_1.csv')

```

The summarytools package can give us a nice overview of our dataset, including the percent of missing values (NAs) recorded in each variable.

```{r view summary, eval = FALSE}
view(dfSummary(df))

```

```{r render summary, eval = TRUE, results = 'asis', style = "grid", echo = FALSE}
# mytable<- dfSummary(df,
#           plain.ascii  = FALSE,
#           style        = 'grid',
#           graph.magnif = 0.85,
#           varnumbers = FALSE,
#           valid.col    = FALSE) #,
#           #tmp.img.dir  = "/tmp")
# 
# knitr::kable(mytable, format = 'html')

print(dfSummary(df, 
                varnumbers   = FALSE,
                valid.col    = FALSE,
                graph.magnif = 0.76),
      method = 'render')

#knitr::kable(mytable, format = 'html')
# print(dfSummary(df, 
#                 varnumbers   = FALSE, 
#                 valid.col    = FALSE, 
#                 graph.magnif = 0.76),
#       method = 'render')

```

Knowing more about the intended sampling design will tell us what other missing values should we look for. For example, were data collected every year? If so examining the years included in the dataset may be important.

Sometimes a visual check is easy to spot what is missing

```{r check for all years}
sort (unique (df$year))
```

Sometimes it's hard to find exactly what is missing! `setdiff` (for simple checks for expected values based on complete sets) `expand.grid` (for checking pre-defined combinations) and the `padr` (for padding out dataset based on regular temporal frequency) are your friends here. As an example, I can use the simple code below to figure out what years are missing. It is then up to you whether it is important to add those years with NA values, or just carry on knowing they are missing.

```{r check for all years using setdiff}
setdiff(seq(min(df$year), max(df$year)), unique(df$year))
```

Commonly, we expect a similar number of samples over some interval (e.g. per site or year)

```{r count by years}
ct_by_year = df %>%
  group_by(year) %>%
  tally()
```

Viewing just the first 10 years, sampling appears very uneven, is this expected?

```{r display ct, echo = FALSE}
knitr::kable(head (ct_by_year, 10), digits = 2)
```
